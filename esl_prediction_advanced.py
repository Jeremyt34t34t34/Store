import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ›´å¥½çœ‹çš„å›¾è¡¨æ ·å¼
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("Set2")
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12

# === 1. è¯»å–å’Œæ¸…æ´—æ•°æ® ===
print("è¯»å–æ•°æ®ä¸­...")
file_path = "DSL Data in Cincinnati.xlsx"
df = pd.read_excel(file_path, sheet_name='å·¥ä½œè¡¨1', header=1)
df.columns = df.columns.astype(str).str.strip()

# === 2. å®šä¹‰ESLåŒºåŸŸåˆ— ===
esl_columns = [
    '1.54"', '2.66"', '4.20"',
    'Central Market 1.54"', 'Central Market 2.66"',
    'Checkout 1.54"', 'Checkout 2.66"',
    'Pharmacy 1.54"', 'Pharmacy 2.66"',
    'Beer 1.54"', 'Beer 2.66"',
    'Cosmetics 1.54"',
    'Dairy 1.54"', 'Dairy 2.66"',
    'Seafood & Meat 1.54"', 'Seafood & Meat 2.66"', 'Seafood & Meat 4.20"',
    'Produce 1.54"', 'Produce 2.66"', 'Produce 4.20"',
    'Bakery 1.54"', 'Bakery 2.66"'
]

# å°†æŒ‡å®šåˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
for col in esl_columns + ['Total', 'Sales SQ FT']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# åˆ›å»ºTotal ESLåˆ—
df['Total ESL'] = df['Total']

# === 3. è®¡ç®—æ¯ä¸ªESLåŒºåŸŸçš„ç™¾åˆ†æ¯” ===
for col in esl_columns:
    df[f"{col} %"] = df[col] / df['Total ESL']

# === 4. æŒ‰åº—é“ºç±»å‹åˆ†ç»„ç»Ÿè®¡ ===
grouped_avg = df.groupby('Major Type')[[f"{col} %" for col in esl_columns]].mean()
grouped_avg.to_excel("ESL_Grouped_Averages_by_Type.xlsx")
print("æŒ‰å•†åº—ç±»å‹åˆ†ç»„çš„å¹³å‡å€¼å·²ä¿å­˜åˆ° ESL_Grouped_Averages_by_Type.xlsx")

# === 5. ç‰¹å¾å·¥ç¨‹ ===
print("\næ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
df_model = df.copy()
# è¿‡æ»¤ç¼ºå¤±å€¼
df_model = df_model[df_model['Sales SQ FT'].notnull() & df_model['Total ESL'].notnull()]

# æ·»åŠ æ–°ç‰¹å¾
df_model['ESL_per_SqFt'] = df_model['Total ESL'] / df_model['Sales SQ FT']
df_model['Log_Sales_SqFt'] = np.log1p(df_model['Sales SQ FT'])
df_model['Sqrt_Sales_SqFt'] = np.sqrt(df_model['Sales SQ FT'])

# åŒºåŸŸè®¡æ•°ç‰¹å¾
df_model['ESL_Regions_Count'] = df_model[esl_columns].count(axis=1)
df_model['ESL_1.54_Total'] = df_model[[col for col in esl_columns if '1.54"' in col]].sum(axis=1)
df_model['ESL_2.66_Total'] = df_model[[col for col in esl_columns if '2.66"' in col]].sum(axis=1)
df_model['ESL_4.20_Total'] = df_model[[col for col in esl_columns if '4.20"' in col]].sum(axis=1)

# åˆ›å»ºè™šæ‹Ÿå˜é‡
df_model = pd.get_dummies(df_model, columns=['Major Type'])

# æ¢ç´¢æ€§æ•°æ®åˆ†æ
print("æ•°æ®å½¢çŠ¶:", df_model.shape)
print("\nä¸»è¦ç»Ÿè®¡é‡:")
print(df_model[['Sales SQ FT', 'Total ESL', 'ESL_per_SqFt']].describe())

# æ£€æµ‹å¼‚å¸¸å€¼ (å¯é€‰æ‹©åç»­å¤„ç†)
Q1 = df_model['Total ESL'].quantile(0.25)
Q3 = df_model['Total ESL'].quantile(0.75)
IQR = Q3 - Q1
outliers = df_model[(df_model['Total ESL'] < (Q1 - 1.5 * IQR)) | (df_model['Total ESL'] > (Q3 + 1.5 * IQR))]
print(f"\næ£€æµ‹åˆ° {len(outliers)} ä¸ªæ½œåœ¨å¼‚å¸¸å€¼")

# === 6. å‡†å¤‡æ¨¡å‹æ•°æ® ===
print("\nå‡†å¤‡æ¨¡å‹æ•°æ®...")
# é€‰æ‹©ç‰¹å¾
feature_cols = ['Sales SQ FT', 'Log_Sales_SqFt', 'Sqrt_Sales_SqFt', 
                'ESL_Regions_Count', 'ESL_1.54_Total', 'ESL_2.66_Total', 'ESL_4.20_Total'] + \
               [col for col in df_model.columns if col.startswith('Major Type_')]

X = df_model[feature_cols]
y = df_model['Total ESL']

# åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"è®­ç»ƒæ•°æ®å¤§å°: {X_train.shape[0]} ä¸ªæ ·æœ¬")
print(f"æµ‹è¯•æ•°æ®å¤§å°: {X_test.shape[0]} ä¸ªæ ·æœ¬")

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# === 7. è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½ ===
print("\nè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ä¸­...")

# åˆ›å»ºç”¨äºæ¯”è¾ƒçš„æ¨¡å‹å­—å…¸
models = {
    "Linear Regression": LinearRegression(),
    "ElasticNet": ElasticNet(alpha=0.1, l1_ratio=0.5),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "XGBoost": xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
}

# å­˜å‚¨ç»“æœ
model_results = []
trained_models = {}

# è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
for name, model in models.items():
    print(f"\næ­£åœ¨è®­ç»ƒ {name} æ¨¡å‹...")
    
    # è®­ç»ƒæ¨¡å‹ - å¯¹äºéƒ¨åˆ†æ¨¡å‹ä½¿ç”¨ç¼©æ”¾æ•°æ®
    if name in ["ElasticNet", "SVR"]:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    # æ‰§è¡Œäº¤å‰éªŒè¯
    if name in ["ElasticNet", "SVR"]:
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
    else:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # å­˜å‚¨ç»“æœ
    result = {
        "Model": name,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "RÂ²": r2,
        "CV RÂ² (mean)": cv_scores.mean(),
        "CV RÂ² (std)": cv_scores.std()
    }
    model_results.append(result)
    trained_models[name] = model
    
    # è¾“å‡ºç»“æœ
    print(f"  MSE: {mse:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  MAE: {mae:.2f}")
    print(f"  RÂ²: {r2:.4f}")
    print(f"  5æŠ˜äº¤å‰éªŒè¯ RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

# åˆ›å»ºæ€§èƒ½æ¯”è¾ƒè¡¨
results_df = pd.DataFrame(model_results)
print("\næ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
print(results_df[['Model', 'RMSE', 'RÂ²', 'CV RÂ² (mean)']].sort_values('RÂ²', ascending=False))

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = results_df.sort_values('RÂ²', ascending=False).iloc[0]['Model']
best_model = trained_models[best_model_name]
print(f"\nâœ“ æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {results_df[results_df['Model'] == best_model_name]['RÂ²'].values[0]:.4f})")

# ä¿å­˜æœ€ä½³æ¨¡å‹
models_dir = "advanced_models"
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
model_filename = f"{models_dir}/esl_prediction_{best_model_name.replace(' ', '_').lower()}_{timestamp}.joblib"

# ä¿å­˜æ¨¡å‹å’Œç¼©æ”¾å™¨
model_data = {
    'model': best_model,
    'scaler': scaler,
    'feature_cols': feature_cols,
    'model_name': best_model_name
}
joblib.dump(model_data, model_filename)
print(f"\nğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {model_filename}")

# === 8. å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ ===
def visualize_model_comparison():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
    # æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
    plt.figure(figsize=(12, 6))
    
    # RÂ² å€¼æ¯”è¾ƒ
    plt.subplot(1, 2, 1)
    sns.barplot(x='Model', y='RÂ²', data=results_df, palette='viridis')
    plt.title('æ¨¡å‹ RÂ² æ¯”è¾ƒ')
    plt.ylim(max(0, results_df['RÂ²'].min() - 0.1), 1.0)
    plt.xticks(rotation=45, ha='right')
    
    # RMSE æ¯”è¾ƒ
    plt.subplot(1, 2, 2)
    sns.barplot(x='Model', y='RMSE', data=results_df, palette='viridis')
    plt.title('æ¨¡å‹ RMSE æ¯”è¾ƒ')
    plt.xticks(rotation=45, ha='right')
    
    plt.tight_layout()
    plt.savefig("Model_Comparison.png", dpi=300, bbox_inches='tight')
    plt.show()
    print("æ¨¡å‹æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜ä¸º Model_Comparison.png")

def visualize_predictions():
    """æ¯”è¾ƒé¢„æµ‹å€¼ä¸å®é™…å€¼"""
    plt.figure(figsize=(12, 6))
    
    # å‡†å¤‡æ•°æ®
    if best_model_name in ["ElasticNet", "SVR"]:
        y_pred = best_model.predict(X_test_scaled)
    else:
        y_pred = best_model.predict(X_test)
    
    # æ•£ç‚¹å›¾
    plt.scatter(y_test, y_pred, alpha=0.7, color='blue')
    
    # ç†æƒ³çº¿
    min_val = min(y_test.min(), y_pred.min())
    max_val = max(y_test.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--')
    
    plt.xlabel('å®é™…å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title(f'{best_model_name} æ¨¡å‹: é¢„æµ‹å€¼ vs å®é™…å€¼')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("Prediction_vs_Actual.png", dpi=300, bbox_inches='tight')
    plt.show()
    print("é¢„æµ‹å€¼ä¸å®é™…å€¼æ¯”è¾ƒå›¾å·²ä¿å­˜ä¸º Prediction_vs_Actual.png")

def visualize_feature_importance():
    """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§ (é€‚ç”¨äºæ ‘æ¨¡å‹)"""
    if best_model_name in ["Random Forest", "Gradient Boosting", "XGBoost"]:
        plt.figure(figsize=(10, 8))
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        if best_model_name == "XGBoost":
            importance = best_model.feature_importances_
        else:
            importance = best_model.feature_importances_
            
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feat_importance = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': importance
        }).sort_values('Importance', ascending=False)
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        sns.barplot(x='Importance', y='Feature', data=feat_importance, palette='viridis')
        plt.title(f'{best_model_name} - ç‰¹å¾é‡è¦æ€§')
        plt.tight_layout()
        plt.savefig("Feature_Importance_Advanced.png", dpi=300, bbox_inches='tight')
        plt.show()
        print("ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜ä¸º Feature_Importance_Advanced.png")
        return feat_importance
    else:
        print(f"æ³¨æ„: {best_model_name} æ¨¡å‹ä¸æä¾›ç›´æ¥çš„ç‰¹å¾é‡è¦æ€§ã€‚")
        return None

def explore_residuals():
    """åˆ†ææ®‹å·®"""
    if best_model_name in ["ElasticNet", "SVR"]:
        y_pred = best_model.predict(X_test_scaled)
    else:
        y_pred = best_model.predict(X_test)
    
    residuals = y_test - y_pred
    
    plt.figure(figsize=(16, 6))
    
    # æ®‹å·®åˆ†å¸ƒ
    plt.subplot(1, 2, 1)
    sns.histplot(residuals, kde=True)
    plt.title('æ®‹å·®åˆ†å¸ƒ')
    plt.xlabel('æ®‹å·®')
    
    # æ®‹å·®ä¸é¢„æµ‹å€¼å…³ç³»
    plt.subplot(1, 2, 2)
    plt.scatter(y_pred, residuals, alpha=0.7)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.title('æ®‹å·® vs é¢„æµ‹å€¼')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    
    plt.tight_layout()
    plt.savefig("Residual_Analysis.png", dpi=300, bbox_inches='tight')
    plt.show()
    print("æ®‹å·®åˆ†æå›¾è¡¨å·²ä¿å­˜ä¸º Residual_Analysis.png")

# === 9. æ”¹è¿›çš„é¢„æµ‹å‡½æ•° ===
def advanced_predict(sales_sqft, major_type=None, model_data=model_data):
    """
    åŸºäºé«˜çº§æ¨¡å‹é¢„æµ‹ESLæ€»æ•°
    
    å‚æ•°:
    ------
    sales_sqft : float
        é”€å”®é¢ç§¯ (å¹³æ–¹è‹±å°º)
    major_type : str, optional
        å•†åº—ç±»å‹ã€‚å¦‚æœä¸ºNoneï¼Œå°†ä½¿ç”¨æ•°æ®é›†ä¸­æœ€å¸¸è§çš„ç±»å‹  
    model_data : dict
        åŒ…å«æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        
    è¿”å›:
    ------
    int
        é¢„æµ‹çš„ESLæ€»æ•°
    """
    # é»˜è®¤ä¸ºæœ€å¸¸è§çš„å•†åº—ç±»å‹
    if major_type is None:
        major_type = df['Major Type'].value_counts().index[0]
        print(f"æœªæŒ‡å®šå•†åº—ç±»å‹ã€‚ä½¿ç”¨æœ€å¸¸è§ç±»å‹: {major_type}")
    
    # æå–æ¨¡å‹å’Œç›¸å…³æ•°æ®
    model = model_data['model']
    scaler = model_data['scaler']
    feature_cols = model_data['feature_cols']
    model_name = model_data['model_name']
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    input_data = {col: 0 for col in feature_cols}
    
    # è®¾ç½®ç‰¹å¾å€¼
    input_data['Sales SQ FT'] = sales_sqft
    input_data['Log_Sales_SqFt'] = np.log1p(sales_sqft)
    input_data['Sqrt_Sales_SqFt'] = np.sqrt(sales_sqft)
    
    # è®¾ç½®è™šæ‹Ÿå˜é‡
    target_col = f'Major Type_{major_type.upper()}'
    if target_col in input_data:
        input_data[target_col] = 1
    else:
        available_types = [col.replace('Major Type_', '') for col in feature_cols if col.startswith('Major Type_')]
        print(f"è­¦å‘Š: å•†åº—ç±»å‹ '{major_type}' åœ¨è®­ç»ƒæ•°æ®ä¸­æœªæ‰¾åˆ°!")
        print(f"å¯ç”¨ç±»å‹: {available_types}")
        return None
    
    # è®¾ç½®åŒºåŸŸè®¡æ•°é»˜è®¤å€¼ (ä½¿ç”¨ç±»ä¼¼å•†åº—çš„å¹³å‡å€¼)
    similar_stores = df[df['Major Type'] == major_type]
    if len(similar_stores) > 0:
        input_data['ESL_Regions_Count'] = similar_stores['Sales SQ FT'].count(axis=1).mean()
        # ä¼°ç®—å„ç§å°ºå¯¸çš„ESLæ€»æ•° (åŸºäºç›¸ä¼¼å•†åº—çš„å¹³å‡å€¼)
        small_esl_cols = [col for col in esl_columns if '1.54"' in col]
        medium_esl_cols = [col for col in esl_columns if '2.66"' in col]
        large_esl_cols = [col for col in esl_columns if '4.20"' in col]
        
        if len(small_esl_cols) > 0:
            input_data['ESL_1.54_Total'] = similar_stores[small_esl_cols].sum(axis=1).mean()
        if len(medium_esl_cols) > 0:
            input_data['ESL_2.66_Total'] = similar_stores[medium_esl_cols].sum(axis=1).mean()
        if len(large_esl_cols) > 0:
            input_data['ESL_4.20_Total'] = similar_stores[large_esl_cols].sum(axis=1).mean()
    else:
        # å¦‚æœæ²¡æœ‰ç›¸ä¼¼å•†åº—ï¼Œä½¿ç”¨æ•´ä½“å¹³å‡å€¼
        input_data['ESL_Regions_Count'] = len(esl_columns)
        input_data['ESL_1.54_Total'] = 0
        input_data['ESL_2.66_Total'] = 0
        input_data['ESL_4.20_Total'] = 0
    
    # åˆ›å»ºè¾“å…¥DataFrame
    x_new = pd.DataFrame([input_data])
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é¢„å¤„ç†æ•°æ®
    if model_name in ["ElasticNet", "SVR"]:
        x_new_processed = scaler.transform(x_new)
    else:
        x_new_processed = x_new
    
    # æ‰§è¡Œé¢„æµ‹
    try:
        prediction = model.predict(x_new_processed)[0]
        return max(0, round(prediction))  # ç¡®ä¿é¢„æµ‹å€¼ä¸ºéè´Ÿæ•´æ•°
    except Exception as e:
        print(f"é¢„æµ‹é”™è¯¯: {e}")
        return None

def advanced_predict_distribution(sales_sqft, major_type=None):
    """
    é¢„æµ‹æ–°å•†åº—çš„ESLåŒºåŸŸåˆ†å¸ƒ
    
    å‚æ•°ä¸advanced_predictç›¸åŒ
    """
    # é¦–å…ˆé¢„æµ‹æ€»ESLæ•°é‡
    total_prediction = advanced_predict(sales_sqft, major_type)
    if total_prediction is None:
        return None
    
    # é»˜è®¤ä¸ºæœ€å¸¸è§çš„å•†åº—ç±»å‹
    if major_type is None:
        major_type = df['Major Type'].value_counts().index[0]
    
    # æ£€æŸ¥å•†åº—ç±»å‹æ˜¯å¦åœ¨åˆ†ç»„å¹³å‡å€¼ä¸­å­˜åœ¨
    try:
        all_types = grouped_avg.index.tolist()
        if major_type.upper() not in all_types:
            print(f"è­¦å‘Š: å•†åº—ç±»å‹ '{major_type}' åœ¨åˆ†ç»„å¹³å‡å€¼ä¸­æœªæ‰¾åˆ°!")
            print(f"å¯ç”¨ç±»å‹: {all_types}")
            return None
        
        # è·å–è¯¥ç±»å‹çš„ç™¾åˆ†æ¯”åˆ†å¸ƒ
        percentages = grouped_avg.loc[major_type.upper()]
        
        # è®¡ç®—å„åŒºåŸŸé¢„æµ‹å€¼
        region_predictions = {}
        for col in percentages.index:
            region_name = col.replace(" %", "")
            region_predictions[region_name] = round(total_prediction * percentages[col])
        
        return region_predictions
    except KeyError:
        print(f"é”™è¯¯: å•†åº—ç±»å‹ {major_type} åœ¨åˆ†ç»„å¹³å‡å€¼ä¸­æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥æ•°æ®ã€‚")
        return None
    except Exception as e:
        print(f"é¢„æµ‹é”™è¯¯: {e}")
        return None

# === 10. äº¤äº’å¼é¢„æµ‹å·¥å…· ===
def run_advanced_prediction():
    """äº¤äº’å¼å·¥å…·ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥å‚æ•°å¹¶æ˜¾ç¤ºé¢„æµ‹ç»“æœä¸å¯è§†åŒ–"""
    # ä¸»å¾ªç¯ - ç›´åˆ°ç”¨æˆ·é€‰æ‹©é€€å‡º
    while True:
        print("\n" + "="*50)
        print("ESL é«˜çº§é¢„æµ‹å·¥å…·")
        print("="*50)
        
        # æ˜¾ç¤ºå¯ç”¨çš„å•†åº—ç±»å‹
        available_types = list(df['Major Type'].unique())
        print("\nå¯ç”¨çš„å•†åº—ç±»å‹:")
        for i, type_name in enumerate(available_types, 1):
            print(f"{i}. {type_name}")
        
        # è·å–ç”¨æˆ·è¾“å…¥
        while True:
            try:
                sales_sqft = float(input("\nè¾“å…¥é”€å”®é¢ç§¯ (Sales SQ FT): "))
                if sales_sqft <= 0:
                    print("é”€å”®é¢ç§¯å¿…é¡»ä¸ºæ­£æ•°! è¯·é‡è¯•ã€‚")
                    continue
                
                print("\né€‰æ‹©å•†åº—ç±»å‹ (è¾“å…¥ç¼–å·æˆ–ç±»å‹åç§°):")
                type_input = input("> ")
                
                # å¤„ç†è¾“å…¥ - å¯ä»¥æ˜¯ç¼–å·æˆ–ç±»å‹åç§°
                selected_type = None
                if type_input.isdigit():
                    idx = int(type_input) - 1
                    if 0 <= idx < len(available_types):
                        selected_type = available_types[idx]
                    else:
                        print(f"æ— æ•ˆç¼–å·! è¯·è¾“å…¥1åˆ°{len(available_types)}ä¹‹é—´çš„æ•°å­—ã€‚")
                        continue
                else:
                    # ç›´æ¥è¾“å…¥ç±»å‹åç§° - æ£€æŸ¥åŒ¹é… (ä¸åŒºåˆ†å¤§å°å†™)
                    for type_name in available_types:
                        if type_input.upper() == type_name.upper():
                            selected_type = type_name
                            break
                    
                    if not selected_type:
                        print("æœªæ‰¾åˆ°è¯¥å•†åº—ç±»å‹! è¯·é‡è¯•ã€‚")
                        continue
                
                break
            except ValueError:
                print("æ— æ•ˆè¾“å…¥! è¯·è¾“å…¥æ•°å­—ã€‚")
        
        # æ˜¾ç¤ºé€‰æ‹©çš„å‚æ•°
        print("\n" + "-"*50)
        print(f"é€‰æ‹©çš„å‚æ•°: é”€å”®é¢ç§¯ = {sales_sqft:.2f} SQ FT | å•†åº—ç±»å‹ = {selected_type}")
        print("-"*50)
        
        # ä½¿ç”¨é«˜çº§æ¨¡å‹ç”Ÿæˆé¢„æµ‹ç»“æœ
        total_prediction = advanced_predict(sales_sqft, major_type=selected_type)
        print(f"\nğŸ”® é¢„æµ‹æ€»ESLæ•°é‡: {total_prediction}")
        
        # é¢„æµ‹ESLåŒºåŸŸåˆ†å¸ƒ
        region_prediction = advanced_predict_distribution(sales_sqft, major_type=selected_type)
        if region_prediction:
            print("\nğŸ”® é¢„æµ‹çš„ESLåŒºåŸŸåˆ†å¸ƒ:")
            # åˆ›å»ºè¡¨æ ¼é£æ ¼è¾“å‡ºï¼Œæé«˜å¯è¯»æ€§
            max_region_length = max(len(region) for region in region_prediction.keys())
            print(f"{'åŒºåŸŸ':<{max_region_length+5}}{'æ•°é‡':<10}{'ç™¾åˆ†æ¯”':<10}")
            print("-" * (max_region_length + 25))
            
            for region, count in region_prediction.items():
                percentage = (count / total_prediction) * 100
                print(f"{region:<{max_region_length+5}}{count:<10}{percentage:.2f}%")
            
            # ç”Ÿæˆå¯è§†åŒ–
            regions = list(region_prediction.keys())
            counts = list(region_prediction.values())
            plt.figure(figsize=(14, 8))
            bars = plt.bar(regions, counts, color=sns.color_palette("viridis", len(regions)))
            plt.xlabel("ESL åŒºåŸŸ", fontweight='bold')
            plt.ylabel("é¢„æµ‹çš„ ESL æ•°é‡", fontweight='bold')
            plt.title(f"é¢„æµ‹çš„ ESL åˆ†å¸ƒ - {selected_type} - {sales_sqft:.0f} SQ FT", fontweight='bold', pad=20)
            plt.xticks(rotation=45, ha='right')
            plt.grid(axis='y', alpha=0.3)
            
            # åœ¨æ¯ä¸ªæŸ±ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 5,
                        f'{int(height)}', ha='center', va='bottom', fontweight='bold', fontsize=9)
            
            plt.tight_layout()
            result_filename = f"Advanced_ESL_Prediction_{selected_type}_{int(sales_sqft)}_SQFT.png"
            plt.savefig(result_filename, dpi=300, bbox_inches='tight')
            plt.show()
            print(f"\né¢„æµ‹å›¾è¡¨å·²ä¿å­˜ä¸º {result_filename}")
        
        # èœå•é€‰é¡¹
        while True:
            print("\næ‚¨æƒ³åšä»€ä¹ˆ?")
            print("1. è¿›è¡Œå¦ä¸€æ¬¡é¢„æµ‹")
            print("2. æŸ¥çœ‹å½“å‰é¢„æµ‹çš„å…¶ä»–åˆ†æ")
            print("3. æŸ¥çœ‹æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
            print("4. é€€å‡ºç¨‹åº")
            
            try:
                choice = int(input("> "))
                if choice == 1:
                    # é€€å‡ºå†…éƒ¨èœå•å¾ªç¯ï¼Œç»§ç»­å¤–éƒ¨é¢„æµ‹å¾ªç¯è¿›è¡Œæ–°é¢„æµ‹
                    break
                elif choice == 2:
                    # æ˜¾ç¤ºå¯è§†åŒ–é€‰é¡¹å­èœå•
                    print("\né€‰æ‹©åˆ†æå›¾è¡¨:")
                    print("1. æ˜¾ç¤ºé¢„æµ‹å€¼ä¸å®é™…å€¼æ¯”è¾ƒ")
                    print("2. æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§åˆ†æ")
                    print("3. æ˜¾ç¤ºæ®‹å·®åˆ†æ")
                    print("4. è¿”å›ä¸»èœå•")
                    
                    try:
                        viz_choice = int(input("> "))
                        if viz_choice == 1:
                            visualize_predictions()
                        elif viz_choice == 2:
                            feature_imp = visualize_feature_importance()
                            if feature_imp is not None:
                                print("\nå‰5ä¸ªæœ€é‡è¦ç‰¹å¾:")
                                print(feature_imp.head(5))
                        elif viz_choice == 3:
                            explore_residuals()
                        # é€‰é¡¹4åªæ˜¯ç»§ç»­å¾ªç¯
                    except (ValueError, IndexError):
                        print("æ— æ•ˆé€‰æ‹©ã€‚è¯·é‡è¯•ã€‚")
                elif choice == 3:
                    visualize_model_comparison()
                elif choice == 4:
                    print("\né€€å‡ºESLé¢„æµ‹å·¥å…·ã€‚å†è§!")
                    return  # å®Œå…¨é€€å‡ºå‡½æ•°
                else:
                    print("æ— æ•ˆé€‰æ‹©ã€‚è¯·é‡è¯•ã€‚")
            except (ValueError, IndexError):
                print("æ— æ•ˆè¾“å…¥ã€‚è¯·è¾“å…¥æ•°å­—ã€‚")

# === 11. åŠ è½½ä¿å­˜çš„æ¨¡å‹ ===
def load_model(model_path=None):
    """åŠ è½½ä¹‹å‰ä¿å­˜çš„æ¨¡å‹æˆ–ä½¿ç”¨å½“å‰æ¨¡å‹"""
    global model_data
    if model_path and os.path.exists(model_path):
        try:
            loaded_model_data = joblib.load(model_path)
            model_data = loaded_model_data
            print(f"å·²ä» {model_path} åŠ è½½æ¨¡å‹")
            return True
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹é”™è¯¯: {e}")
            return False
    return False

# === 12. å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒå’Œæ€§èƒ½åˆ†æ ===
print("\nç”Ÿæˆæ¨¡å‹æ¯”è¾ƒå¯è§†åŒ–...")
visualize_model_comparison()
visualize_predictions()
feature_importance = visualize_feature_importance()
if feature_importance is not None:
    print("\næœ€é‡è¦çš„5ä¸ªç‰¹å¾:")
    print(feature_importance.head(5))
explore_residuals()

# === 13. æ‰§è¡Œäº¤äº’å¼é¢„æµ‹ ===
if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦åº”åŠ è½½ä¿å­˜çš„æ¨¡å‹
    if os.path.exists("advanced_models"):
        model_files = [f for f in os.listdir("advanced_models") if f.endswith(".joblib")]
        if model_files:
            print("\næ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹:")
            for i, model_file in enumerate(model_files, 1):
                print(f"{i}. {model_file}")
            print(f"{len(model_files)+1}. ä½¿ç”¨å½“å‰æ¨¡å‹")
            
            try:
                choice = int(input("\né€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹: "))
                if 1 <= choice <= len(model_files):
                    load_model(os.path.join("advanced_models", model_files[choice-1]))
            except (ValueError, IndexError):
                print("æ— æ•ˆé€‰æ‹©ã€‚ä½¿ç”¨å½“å‰æ¨¡å‹ã€‚")
    
    # è¿è¡Œäº¤äº’å¼é¢„æµ‹
    print("\nå¯åŠ¨äº¤äº’å¼é¢„æµ‹å·¥å…·...")
    run_advanced_prediction() 